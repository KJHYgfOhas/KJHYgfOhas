{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyclic Orthogonal Convolutions\n",
    "\n",
    "The aim of this notebook is to present the main results of the paper: \"Cyclic orthogonal convolutions for long-rangeintegration of features\".\n",
    "\n",
    "It is divided in four main sections corresponding to the three main experiments studied in the paper.\n",
    "\n",
    "- CIFAR-10\n",
    "- ImageNet, Stylized-ImageNet and Cue Conflict\n",
    "- Receptive fields\n",
    "- Pathfinder\n",
    "\n",
    "In each section, the user can load a pre-trained model and evaluate its performance.\n",
    "A few of the models showed in the paper are available in this repository, one for each set of parameters.\n",
    "This notebook is intended be used for evaluation only, not for training. \n",
    "\n",
    "Note that the code runs on GPU. This notebook runs on Tensorflow 1.15.0 and Python 3.6.8.\n",
    "\n",
    "When loading a model, the option model=\"CNN\" refers to a standard convolutional network, while model=\"CycleNet\" corresponds to the new proposed architecture. Other parameters corresponds to those explained in the paper: 'depth' is the number of layers, 'kernel' is the kernel size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and setting up the environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.23.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.15.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.17.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.5.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from layers.CNNCycle import CNNCycle\n",
    "from layers.BilinearInterpolate3D import BilinearInterpolate3D\n",
    "from layers.CycleNetCycle import NetXCycle\n",
    "from image_classification.imagenet.helpers.imagenet_dataset import get_dataset\n",
    "from image_classification.imagenet.helpers.imagenet_helpers import top_1_accuracy, top_5_accuracy\n",
    "from image_classification.cifar10.helpers.cifar10_dataset import *\n",
    "from saliency.helpers.saliency import *\n",
    "from image_classification.imagenet.helpers.cue_conflict_mapping import superclasses, superclasses_names, predicted_superclass\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting a GPU memory fraction used (if desired) to limit resource usage\n",
    "# gpu_memory_fraction = 0.4\n",
    "# gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n",
    "# tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set larger batch size to speed up evaluation times, or smaller to avoid OOM issues.\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10\n",
    "\n",
    "- Load dataset\n",
    "- Load model\n",
    "- Show model summary\n",
    "- Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset path and load dataset\n",
    "cifar_dataset_path = 'image_classification/cifar10/data/CIFAR-10_test.h5'\n",
    "x_test, y_test = read_h5_dataset(cifar_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /tf/MtkResearch/KJHYgfOhas/Submission/layers/BilinearInterpolate3D.py:20: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load desired model\n",
    "model = \"CNN\"    # \"CycleNet\" or \"CNN\"\n",
    "depth = 3          # 3, 6, 9, 12, 15 or 18\n",
    "cifar_model_path = 'image_classification/cifar10/models/'+model+'_models/'+model+'_cifar10_depth'+str(depth)+'.h5'\n",
    "cifar_model = load_model(cifar_model_path, custom_objects={'BilinearInterpolate3D': BilinearInterpolate3D})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 32, 80)        2240      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 32, 32, 80)        320       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 80)        0         \n",
      "_________________________________________________________________\n",
      "bilinear_interpolate3d (Bili (None, 5, 5, 5)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 5, 5, 5)           230       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 5, 5, 5)           20        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5, 5, 5)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 5, 5, 5)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 5, 5, 5)           230       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 5, 5, 5)           20        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5, 5, 5)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 5, 5)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 5, 5)           230       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 5, 5, 5)           20        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 5, 5, 5)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 5, 5)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 125)               0         \n",
      "_________________________________________________________________\n",
      "output_0 (Dense)             (None, 10)                1260      \n",
      "=================================================================\n",
      "Total params: 4,570\n",
      "Trainable params: 4,380\n",
      "Non-trainable params: 190\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "cifar_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 134us/sample - loss: 1.5254 - acc: 0.4537\n"
     ]
    }
   ],
   "source": [
    "# Evaluating \n",
    "metrics_cifar = cifar_model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet, Stylized-ImageNet and Cue Conflict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:red;font-size:17px;\">Note: Due to copyright restrictions, it is not possible for us to share the ImageNet and Stylized Imagenet dataset.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to source the ImageNet dataset online (at http://image-net.org/download) and the Stylized-ImageNet dataset can be generated following the instructions and scripts at https://github.com/rgeirhos/Stylized-ImageNet.\n",
    "In order to load the datasets for training and/or evaluation we use the tensorflow records format.\n",
    "We provide the scripts that can be used to convert the ImageNet and Stylized-Imagenet datasets to this format, these are located in the image_classification/imagenet/helpers/convert_tfrecords folder in this repository.\n",
    "Each of the models trained on ImageNet can be used for testing on: ImageNet, Stylized-ImageNet and Cue conflict.\n",
    "\n",
    "- Show model summary\n",
    "- Load model from a directory\n",
    "\n",
    "Then for each dataset:\n",
    "- Initialize dataset generator\n",
    "- Evaluate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_IMAGE_SIZE = 128\n",
    "\n",
    "# Load model\n",
    "nr_initial_CNN_cycles = 0                 # a number between 0 and 5 (included)\n",
    "imagenet_model_path = \"image_classification/imagenet/models/CNN-CycleNet_models/CNN\"+str(nr_initial_CNN_cycles)+\"-CycleNet\"+str(5-nr_initial_CNN_cycles)+\"_imagenet.h5\" \n",
    "imagenet_model = load_model(imagenet_model_path, custom_objects={'NetXCycle': NetXCycle, 'BilinearInterpolate3D': BilinearInterpolate3D, 'CNNCycle': CNNCycle, \"top_1_accuracy\": top_1_accuracy, \"top_5_accuracy\": top_5_accuracy, \"tf\": tf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 128, 128, 128)     3584      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "bilinear_interpolate3d (Bili (None, 106, 106, 106)     0         \n",
      "_________________________________________________________________\n",
      "net_x_cycle (NetXCycle)      (None, 106, 106, 106)     304962    \n",
      "_________________________________________________________________\n",
      "bilinear_interpolate3d_1 (Bi (None, 106, 106, 106)     0         \n",
      "_________________________________________________________________\n",
      "net_x_cycle_1 (NetXCycle)    (None, 106, 106, 106)     304962    \n",
      "_________________________________________________________________\n",
      "bilinear_interpolate3d_2 (Bi (None, 106, 106, 106)     0         \n",
      "_________________________________________________________________\n",
      "net_x_cycle_2 (NetXCycle)    (None, 106, 106, 106)     304962    \n",
      "_________________________________________________________________\n",
      "bilinear_interpolate3d_3 (Bi (None, 106, 106, 106)     0         \n",
      "_________________________________________________________________\n",
      "net_x_cycle_3 (NetXCycle)    (None, 106, 106, 106)     304962    \n",
      "_________________________________________________________________\n",
      "bilinear_interpolate3d_4 (Bi (None, 12, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "net_x_cycle_4 (NetXCycle)    (None, 12, 12, 12)        4068      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1728)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              1729000   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1000)              0         \n",
      "=================================================================\n",
      "Total params: 2,957,012\n",
      "Trainable params: 2,954,140\n",
      "Non-trainable params: 2,872\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "imagenet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tf/MtkResearch/KJHYgfOhas/Submission/image_classification/imagenet/helpers/imagenet_dataset.py:88: The name tf.matching_files is deprecated. Please use tf.io.matching_files instead.\n",
      "\n",
      "WARNING:tensorflow:From /tf/MtkResearch/KJHYgfOhas/Submission/image_classification/imagenet/helpers/imagenet_dataset.py:100: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.rint is deprecated. Please use tf.math.rint instead.\n",
      "\n",
      "WARNING:tensorflow:From /tf/MtkResearch/KJHYgfOhas/Submission/image_classification/imagenet/helpers/imagenet_image_processing.py:63: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /tf/MtkResearch/KJHYgfOhas/Submission/image_classification/imagenet/helpers/imagenet_image_processing.py:70: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "# Initialise the dataset generator for ImageNet\n",
    "imagenet_dataset_path = 'path/to/ImageNet/dataset' # substitute here the path to the imagenet dataset in tensorflow records format\n",
    "ds_valid_imagenet = get_dataset(imagenet_dataset_path, 'validation', BATCH_SIZE, image_size=IMAGENET_IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 297s 381ms/step - loss: 2.2466 - acc: 0.4919 - top_1_accuracy: 0.4919 - top_5_accuracy: 0.7386\n"
     ]
    }
   ],
   "source": [
    "# Evaluating\n",
    "metrics_imagenet = imagenet_model.evaluate(x=ds_valid_imagenet, steps=50000 // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stylized ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the dataset generator for Stylized-ImageNet\n",
    "stylized_imagenet_dataset_path = 'path/to/Stylized-ImageNet/dataset' # substitute here the path to the imagenet dataset in tensorflow records format\n",
    "ds_valid_stylized = get_dataset(stylized_imagenet_dataset_path, 'validation', BATCH_SIZE, image_size=IMAGENET_IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 300s 384ms/step - loss: 8.6678 - acc: 0.0318 - top_1_accuracy: 0.0318 - top_5_accuracy: 0.0890\n"
     ]
    }
   ],
   "source": [
    "# Evaluating\n",
    "metrics_stylized = imagenet_model.evaluate(x=ds_valid_stylized, steps=50000 // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cue Conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvf image_classification/imagenet/data/cue_conflict/cue_conflict_128.tar.xz\n",
    "cue_conflict_dataset_path = 'image_classification/imagenet/data/cue_conflict/cue_conflict_128.pkl'\n",
    "\n",
    "# Load cue conflict\n",
    "with open(cue_conflict_dataset_path,'rb') as f:\n",
    "    cc_X, cc_y_true = pickle.load(f)\n",
    "!rm cue_conflict_128.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cue conflict dataset accuracy for the selected model is: 0.1640625\n"
     ]
    }
   ],
   "source": [
    "# Evaluating\n",
    "cc_y_pred = list([predicted_superclass(imagenet_class_probabilities.tolist())[1] for imagenet_class_probabilities in imagenet_model.predict(cc_X)]) \n",
    "cue_conflict_accuracy = accuracy_score(cc_y_true, cc_y_pred)\n",
    "print(\"The cue conflict dataset accuracy for the selected model is:\" , cue_conflict_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receptive fields plots\n",
    "- Load and scale a sample image\n",
    "- Choose layer list and number of nodes to sample\n",
    "- Plot receptive field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7eff589c1b70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and scale a sample image at random\n",
    "image_index = np.random.randint(0, x_test.shape[0]-1)\n",
    "image = x_test[image_index]\n",
    "plt.imshow((image * NORMALIZING_CONST + CENTRALIZING_CONST)/QUANTIZATION_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose two layers to visualize the receptive field.\n",
    "\n",
    "Default is:\n",
    "- After first convolution (layer number = 1)\n",
    "- After last cycle (layer number = 15+(depth/3-1)*16)\n",
    "\n",
    "Change numbers in 'layer_list' to visualize receptive fields in other layers.\n",
    "Add more elements to 'layer_list' to visualize more layers.\n",
    "\n",
    "'no_of_nodes' is the number of neurons for which the receptive field is visualized, in each layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose layer list and number of nodes to sample\n",
    "no_of_nodes = 3\n",
    "layer_list = [1, int(15+(depth/3-1)*16)]   # Add instructions on which layer is being checked\n",
    "saliency = get_saliency_maps_and_fitted_ellipses(cifar_model, layer_list, no_of_nodes, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot receptive field. If a node is deactivated, a blank image will be displayed.\n",
    "print_images(*saliency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receptive fields stats\n",
    "- Choose a layer (default is last cycle), number of nodes and images to sample.\n",
    "- Compute and display receptive field statistics. Deactivated nodes do not participate in the statistics.\n",
    "\n",
    "The output displays the mean and standard deviation of the receptive field size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables controlling layer number, number of images, number of nodes and image indices.\n",
    "layer = int(15+(depth/3-1)*16)\n",
    "no_of_images = 2\n",
    "no_of_nodes = 2\n",
    "image_indices = np.random.randint(0, x_test.shape[0]-1, size=no_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 12.692104171753089\n",
      "Std: 0.9971108365644188\n"
     ]
    }
   ],
   "source": [
    "# Compute statistics \n",
    "sqrt_traces = get_saliency_stats(cifar_model, layer, no_of_nodes, x_test[image_indices])\n",
    "print(\"Mean:\", np.mean(sqrt_traces))\n",
    "print(\"Std:\", np.std(sqrt_traces))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pathfinder\n",
    "\n",
    "The orginal test dataset included 100000 images, following the test-training split of the original pathfinder paper https://arxiv.org/abs/1805.08315. To prevent possible memory problems that the user my encounter while loading the data, here we provide a subset of 10000 random images of the test data employed for the evaluation. \n",
    "\n",
    "- Load dataset\n",
    "- Set model from a directory\n",
    "- (Optional) For large models of kernel size 20, run a script to untar and reconstruct the model.\n",
    "- Load model\n",
    "- Show model summary\n",
    "- Evaluate model\n",
    "\n",
    "'length' refers to the the path lengths of the pathfinder dataset (see paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"CycleNet\"    # \"CycleNet\" or \"CNN\"\n",
    "length = 9              # 6, 9 or 14\n",
    "kernel = 8              # 4, 8, 12 or 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathfinder_dataset_path = 'pathfinder/data/pathfinder_path'+str(length)+'_test.h5'\n",
    "x_test, y_test = read_h5_dataset(pathfinder_dataset_path, scale = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model path (either directory or h5 file) - no trailing `/` for kernel size 20, otherwise the .h5 file directly\n",
    "pathfinder_model_path = 'pathfinder/models/'+model+'_models/'+model+'_pathfinder'+str(length)+'_kernelsize'+str(kernel)\n",
    "\n",
    "# Identify if the path given identifies a h5 file or a directory requiring untarring\n",
    "# If untarring required, extract model and update model path to extracted h5 file\n",
    "if kernel==20:\n",
    "    !sh pathfinder/helpers/untar.sh $pathfinder_model_path\n",
    "    a = !ls $pathfinder_model_path\n",
    "    model = [i for i in a if i.find('.h5') != -1][0]\n",
    "    pathfinder_model_path = pathfinder_model_path + '/' + model\n",
    "else:\n",
    "    pathfinder_model_path += '.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "pathfinder_model = load_model(pathfinder_model_path, custom_objects={'BilinearInterpolate3D':BilinearInterpolate3D,\n",
    "                                                                     'NetXCycle': NetXCycle})\n",
    "# Remove extracted model file if required\n",
    "if kernel==20:\n",
    "    !rm $pathfinder_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 128, 128, 128)     8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "net_x_cycle (NetXCycle)      (None, 128, 128, 128)     3147648   \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,156,609\n",
      "Trainable params: 3,155,585\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "pathfinder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the orthogonal convolution cycle is defined expicitly for the CIFAR models. 'model.summary()' shows the sequence of convolution and permutations; for the pathfinder models, instead, the cycles are wrapped using a \"NetXCycle\" layer. The CIFAR models are defined explicitly to allow computing the receptive fields at different stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 25s 3ms/sample - loss: 0.2660 - acc: 0.8902\n"
     ]
    }
   ],
   "source": [
    "# Evaluating \n",
    "scores = pathfinder_model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
